% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{tabularx}
\let\oldtabular\tabular
\let\endoldtabular\endtabular
\renewenvironment{tabular}{\tiny\oldtabular}{\endoldtabular}
\usepackage{titling}
\usepackage{titlesec}
\pretitle{\begin{center}\LARGE\bfseries}
\posttitle{\end{center}\vspace{0.5em}}
\preauthor{\begin{center}\large}
\postauthor{\end{center}}
\predate{\begin{center}\large}
\postdate{\end{center}\vspace{2em}}
\renewcommand{\maketitlehookd}{\vspace{1em}\begin{center}\footnotesize -.-. . .-. . -... .-. ..- -- ---... / -.-. .- ... . -....- . -. .- -... .-.. . -.. / .-. . .- ... --- -. .. -. --. / . -. --. .. -. . / .-- .. - .... / -... .- -.-- . ... .. .- -. / .-. . .--. .-. . ... . -. - .- - .. --- -. ... / ..-. --- .-. / ..- -. .. ..-. .. . -.. / -- --- -.. . .-.. .. -. --. \end{center}\vspace{1em}}
\usepackage{hyperref}
\AtBeginDocument{\let\oldmaketitle\maketitle\renewcommand{\maketitle}{\oldmaketitle\begin{center}\large DOI: \href{https://doi.org/10.5281/zenodo.15170908}{10.5281/zenodo.15170908}\end{center}\maketitlehookd}}
\usepackage{appendix}
\renewcommand{\appendixname}{Appendix}

\author{}
\date{}

\begin{document}
\frontmatter

\mainmatter
\hypertarget{appendix-1-mathematical-formalization}{%
\chapter{Appendix 1: Mathematical
Formalization}\label{appendix-1-mathematical-formalization}}

\hypertarget{mathematical-appendix}{%
\section{Mathematical Appendix}\label{mathematical-appendix}}

This appendix contains all mathematical formalizations referenced
throughout the paper, organized by equation number.

\hypertarget{variational-free-energy-and-case-transformations}{%
\subsection{Variational Free Energy and Case
Transformations}\label{variational-free-energy-and-case-transformations}}

\textbf{Equation 1: Variational Free Energy for Case Transformation}

\[
F = D_{KL}[q(s|T(m))||p(s|m)] - \mathbb{E}_{p}[\log p(o|s,T(m))]  \tag{1}
\]

where T(m) represents the transformed model, s are internal states, and
o are observations.

\textbf{Equation 2: Markov Blanket and Case Relationship}

\[\text{Case}(M) \subseteq \text{MB}(M)  \tag{2}\]

where MB(M) denotes the Markov blanket of model M.

\textbf{Equation 3: Precision Weighting for Case Selection}

\[\beta(c,m) = \frac{\exp(-F(c,m))}{\sum_{i}\exp(-F(c_i,m))}  \tag{3}\]

where β(c,m) is the precision weight for case c and model m.

\textbf{Equation 4: Case-Specific Gradient Descent on Free Energy}

\[\frac{\partial m}{\partial t} = -\kappa_c \cdot \frac{\partial F}{\partial m}  \tag{4}\]

where \(\kappa_c\) is the case-specific learning rate.

\textbf{Equation 5: Expected Free Energy Reduction in Case Transitions}

\[
\mathbb{E}[\Delta F] = \sum_{s,a}T(s'|s,a)\pi[a|s](F(s,c)-F(s',c'))  \tag{5}
\]

where c and c' represent the initial and target cases respectively.

\textbf{Equation 6: Bayes Factor for Case Selection}

\[BF = \frac{p(o|m,c_1)}{p(o|m,c_2)}  \tag{6}\]

\textbf{Equation 7: Free Energy Minimization in Case Transitions}

\[
F = D_{KL}[q(s|c,m) || p(s|m)] - \mathbb{E}_{q(s|c,m)}[\log p(o|s,c,m)]  \tag{7}
\]

\hypertarget{message-passing-rules-for-different-cases}{%
\subsection{Message Passing Rules for Different
Cases}\label{message-passing-rules-for-different-cases}}

These equations illustrate how case assignments modulate standard
hierarchical message passing (e.g., in predictive coding) where
beliefs/predictions (\(\mu\)) and prediction errors (\(\varepsilon\))
flow between adjacent levels (denoted by superscripts 0 and 1). The
case-specific weights (\(\kappa_c\)) determine the influence of each
message type based on the model's current functional role.

\textbf{Equations 8-12: Case-Specific Message Passing Rules}

\[\text{Nominative [NOM]}: \mu^0 = \mu^0 + \kappa_{NOM} \cdot (\mu^1 - \mu^0)  \tag{8}\]
\emph{(Lower-level prediction \(\mu^0\) updated by top-down prediction
\(\mu^1\), weighted by \(\kappa_{NOM}\))}

\[\text{Accusative [ACC]}: \varepsilon^1 = \varepsilon^1 + \kappa_{ACC} \cdot (\varepsilon^0 - \varepsilon^1)  \tag{9}\]
\emph{(Higher-level error \(\varepsilon^1\) updated by bottom-up error
\(\varepsilon^0\), weighted by \(\kappa_{ACC}\))}

\[\text{Dative [DAT]}: \mu^0 = \mu^0 + \kappa_{DAT} \cdot (data - \mu^0)  \tag{10}\]
\emph{(Lower-level prediction \(\mu^0\) updated directly by incoming
`data', weighted by \(\kappa_{DAT}\))}

\[\text{Genitive [GEN]}: output = \mu^0 + \kappa_{GEN} \cdot \eta  \tag{11}\]
\emph{(Output generated based on lower-level prediction \(\mu^0\),
weighted by \(\kappa_{GEN}\), potentially with noise \(\eta\))}

\[\text{Instrumental [INS]}: process = f(\mu^1, \varepsilon^0) \cdot \kappa_{INS} \tag{12}\]

\emph{(A process output determined by some function \(f\) of top-down
prediction \(\mu^1\) and bottom-up error \(\varepsilon^0\), weighted by
\(\kappa_{INS}\))}

\[\text{Vocative [VOC]}: activation = \sigma(\kappa_{VOC} \cdot sim(id, address)) \tag{12a}\]

\emph{(Activation state determined by similarity between model identity
\(id\) and incoming address, weighted by \(\kappa_{VOC}\) and passed
through activation function \(\sigma\))}

where \(\kappa_c\) represents case-specific learning rates or precision
weights, \(\eta\) is a noise term, \(\mu^0, \mu^1\) represent
beliefs/predictions, and \(\varepsilon^0, \varepsilon^1\) represent
prediction errors at adjacent hierarchical levels.

\hypertarget{precision-allocation-and-resource-optimization}{%
\subsection{Precision Allocation and Resource
Optimization}\label{precision-allocation-and-resource-optimization}}

\textbf{Equation 13: Precision Weight Allocation with Temperature}

\[\beta(c,m) = \frac{\exp(-\gamma \cdot F(c,m))}{\sum_i \exp(-\gamma \cdot F(c_i,m))}  \tag{13}\]

where γ is the inverse temperature parameter controlling allocation
sharpness.

\textbf{Equation 14: Resource-Weighted Free Energy}

\[F_{\beta}(m) = \sum_c \beta(c,m) \cdot F(c,m) \cdot R(c)  \tag{14}\]

where R(c) represents the computational resources allocated to case c.

\hypertarget{novel-case-formalizations}{%
\subsection{Novel Case Formalizations}\label{novel-case-formalizations}}

\textbf{Equation 15: Conjunctive Case Free Energy}

\[
F_{CNJ} = D_{KL}[q(s|CNJ,m) || p(s|m)] - \mathbb{E}_{q(s|CNJ,m)}[\log p(o|s,\{m_i\})]  \tag{15}
\]

where \{m\_i\} represents the assembly of connected models.

\textbf{Equation 16: Conjunctive Case Message Passing}

\[\mu^{CNJ} = \sum_i w_i \cdot \mu_i + \kappa_{CNJ} \cdot (\prod_i \mu_i - \sum_i w_i \cdot \mu_i)  \tag{16}\]

where w\_i are model-specific weighting factors.

\textbf{Equation 17: Recursive Case Precision Dynamics}

\[\beta(REC,m) = \frac{\exp(-\gamma \cdot F(REC,m))}{\sum_i \exp(-\gamma \cdot F(c_i,m)) + \exp(-\gamma \cdot F(REC,m))}  \tag{17}\]

\hypertarget{glossary-of-variables}{%
\subsubsection{Glossary of Variables}\label{glossary-of-variables}}

\begin{itemize}
\tightlist
\item
  \(a\): Action (in MDP context, often selecting a case transition)
\item
  \(\alpha\): Learning rate (in Neural Process Models context)
\item
  \(BF\): Bayes Factor (for comparing model evidence between cases)
\item
  \(c, c_i, c', c_1, c_2\): Linguistic case assignment (e.g., NOM, ACC,
  specific case instances)
\item
  \(\text{Case}(M)\): Case assignment of model \(M\)
\item
  \textbf{Case Transformation}: An operation that changes the functional
  role (case) of a model within the system
\item
  \textbf{CEREBRUM}: Case-Enabled Reasoning Engine with Bayesian
  Representations for Unified Modeling
\item
  \(D_{KL}\): Kullback-Leibler divergence
\item
  \(\text{data}\): Input data (in Dative case message passing; Eq 10)
\item
  \textbf{Declinability}: The capacity of a generative model within
  CEREBRUM to assume different morphological and functional roles
  (cases) through transformations
\item
  \(E_p[\cdot]\): Expectation with respect to distribution \(p\)
  (Information Geometry)
\item
  \(\mathbb{E}[\cdot]\): Expectation operator
\item
  \(F\): Variational Free Energy
\item
  \(F_{\beta}(m)\): Resource-weighted free energy for model \(m\)
\item
  \(F_{CNJ}\): Free energy for the speculative Conjunctive case
\item
  \(f(...)\): Function (used generally; e.g., in Instrumental message
  passing; Eq 12)
\item
  \(g_{ij}\): Fisher information metric tensor component (Information
  Geometry)
\item
  \(i, j\): Indices for summation or tensor components
\item
  \(L(M)\): Lyapunov function for model \(M\) (Dynamical Systems
  section)
\item
  \(m, M\): Cognitive model
\item
  \(\{m_i\}\): Assembly or set of connected models
\item
  \(\text{MB}(M)\): Markov blanket of model \(M\)
\item
  \textbf{Morphological Marker (Computational Analogue)}: Specific
  computational properties (e.g., active interfaces; parameter access
  patterns; update dynamics) that signal a model's current case
  assignment within CEREBRUM
\item
  \(n\): Model parameter count (Complexity section)
\item
  \(O(...)\): Big O notation for computational complexity
\item
  \(o\): Observations or sensory data
\item
  \(\text{output}\): Output generated by a model (in Genitive case; Eq
  11)
\item
  \(p(s|...)\): Prior distribution over internal states \(s\)
\item
  \(p(o|...)\): Likelihood distribution of observations \(o\)
\item
  \(p(x|theta)\): Probability distribution of data \(x\) given
  parameters \(theta\) (Information Geometry)
\item
  \(\text{process}\): Result of a process executed by a model (in
  Instrumental case; Eq 12)
\item
  \(q(s|...)\): Approximate posterior distribution over internal states
  \(s\)
\item
  \(R(c)\): Computational resources allocated to case \(c\)
\item
  \(REC\): Speculative Recursive case assignment
\item
  \(s\): Internal states of a model
\item
  \(s'\): Next state (in MDP context; target case assignment)
\item
  \(t\): Time variable (in gradient descent context; Eq 4)
\item
  \(T\): Transformation function (e.g., \(T(m)\) is a transformed model
  in Eq 1; also MDP transition function)
\item
  \(T(s'|s,a)\): State transition function in MDP (probability of
  transitioning to state \(s'\) from state \(s\) given action \(a\))
\item
  \(w_i\): Model-specific weighting factors (in Conjunctive case; Eq 16)
\item
  \(\Delta F\): Change in Free Energy
\item
  \(\Delta w_{ij}\): Change in synaptic weight between neuron \(i\) and
  \(j\) (Neural Process Models section)
\item
  \(\beta(c,m)\): Precision weight (allocation) assigned to model \(m\)
  in case \(c\)
\item
  \(\gamma\): Inverse temperature parameter (controlling precision
  allocation sharpness)
\item
  \(\epsilon_i\): Error signal of neuron \(i\) (Neural Process Models
  section)
\item
  \(\varepsilon^0, \varepsilon^1\): Error signals used in message
  passing (representing prediction errors at adjacent hierarchical
  levels; Eq 9, 12)
\item
  \(\eta\): Noise term (Eq 11)
\item
  \(\kappa_c\): Case-specific learning rate or precision weight
  (modulating message updates; Eqs 4, 8-12)
\item
  \(\mu^0, \mu^1\): Mean values used in message passing (representing
  predictions or beliefs at adjacent hierarchical levels)
\item
  \(\mu^{CNJ}\): Mean value resulting from Conjunctive case message
  passing
\item
  \(\pi(a|s)\): Policy in MDP (probability of taking action \(a\) in
  state \(s\))
\item
  \(\sigma'(a_j)\): Derivative of activation function of neuron \(j\)
  (Neural Process Models section)
\item
  \(theta, theta_i, theta_j\): Model parameters \# Appendix 2: Novel
  Linguistic Cases
\end{itemize}

\hypertarget{discovering-and-creating-new-linguistic-cases-through-cerebrum}{%
\section{Discovering and Creating New Linguistic Cases Through
CEREBRUM}\label{discovering-and-creating-new-linguistic-cases-through-cerebrum}}

The CEREBRUM framework not only operationalizes traditional linguistic
cases but potentially enables the discovery of entirely new case
archetypes through its systematic approach to model interactions. As
cognitive models interact in increasingly complex ecosystems, emergent
functional roles may arise that transcend the classical case system
derived from human languages.

\hypertarget{emergence-of-novel-case-functions}{%
\subsection{Emergence of Novel Case
Functions}\label{emergence-of-novel-case-functions}}

Traditional linguistic case systems evolved to serve human communication
needs in physical and social environments. However, computational
cognitive ecosystems face novel challenges and opportunities that may
drive the emergence of new functional roles. The mathematical formalism
of CEREBRUM provides a scaffold for identifying these emergent case
functions through:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Pattern detection in model interaction graphs}: Recurring
  patterns of information flow that don't fit established cases
\item
  \textbf{Free energy anomalies}: Unusual optimization patterns
  indicating novel functional configurations
\item
  \textbf{Precision allocation clusters}: Statistical clustering of
  precision weightings revealing new functional categories
\item
  \textbf{Transition probability densities}: Dense regions in case
  transition probability spaces suggesting stable new cases
\end{enumerate}

\hypertarget{speculative-novel-case-the-emergent-conjunctive-case}{%
\subsection{Speculative Novel Case: The Emergent ``Conjunctive''
Case}\label{speculative-novel-case-the-emergent-conjunctive-case}}

One speculative example of a novel case that might emerge within
CEREBRUM is what we might term the ``conjunctive'' case {[}CNJ{]}. This
case would represent a model's role in synthesizing multiple predictive
streams into coherent joint predictions that couldn't be achieved
through simple composition of existing cases.

The mathematical formalism for a model in conjunctive case would extend
the standard free energy equation as shown in Equation 15 (see
Mathematical Appendix), representing the assembly of connected models
participating in the joint prediction. The key innovation is that the
likelihood term explicitly depends on multiple models' predictions
rather than a single model's output, enabling integration of diverse
predictive streams.

In the message-passing formulation, the conjunctive case would introduce
unique update rules as described in Equation 16 (see Mathematical
Appendix), with weighting factors for individual model predictions, as
well as a multiplicative integration of predictions that captures
interdependencies beyond simple weighted averaging. This formulation
enables rich joint inference across model collectives.

\hypertarget{speculative-novel-case-the-recursive-case}{%
\subsection{Speculative Novel Case: The ``Recursive''
Case}\label{speculative-novel-case-the-recursive-case}}

Another potential novel case is the ``recursive'' case {[}REC{]}, which
would enable a model to apply its transformations to itself, creating a
form of computational reflection not captured by traditional cases.

In the recursive case, a model assumes both agent and object roles
simultaneously, creating feedback loops that enable complex
self-modification behaviors. This case would be particularly relevant
for metalearning systems and artificial neural networks that modify
their own architectures.

The recursive case would introduce unique precision dynamics as
formalized in Equation 17 (see Mathematical Appendix). The key
innovation is that the model appears on both sides of the
transformation, creating a form of self-reference that traditional case
systems don't accommodate. This enables models to introspect and modify
their own parameters through self-directed transformations.

\hypertarget{speculative-novel-case-the-metaphorical-case}{%
\subsection{Speculative Novel Case: The ``Metaphorical''
Case}\label{speculative-novel-case-the-metaphorical-case}}

A third potential novel case is the ``metaphorical'' case {[}MET{]},
which would enable a model to map structures and relationships from one
domain to another, creating computational analogies that transfer
knowledge across conceptual spaces.

In the metaphorical case, a model acts as a transformation bridge
between disparate domains, establishing systematic mappings between
conceptual structures. This case would be particularly valuable for
transfer learning systems and creative problem-solving algorithms that
need to apply learned patterns in novel contexts.

The metaphorical case would introduce unique cross-domain mapping
functions as formalized in Equation 18 (see Mathematical Appendix). The
key innovation is the structured alignment of latent representations
across domains, enabling principled knowledge transfer that preserves
relational invariants while adapting to target domain constraints.

\hypertarget{connections-to-human-cognition-and-communication}{%
\subsubsection{Connections to Human Cognition and
Communication}\label{connections-to-human-cognition-and-communication}}

The metaphorical case has rich connections to multiple domains of human
cognition and communication. In affective neuroscience, it models how
emotional experiences are mapped onto conceptual frameworks, explaining
how we understand emotions through bodily metaphors (e.g., ``heavy
heart,'' ``burning anger''). In first and second-person neuroscience,
metaphorical mappings enable perspective-taking and empathy through
systematic projection of one's own experiential models onto others.
Educational contexts leverage metaphorical case operations when complex
concepts are taught through familiar analogies, making abstract ideas
concrete through structured mappings. The way people converse about
generative models often employs metaphorical language---describing
models as ``thinking,'' ``imagining,'' or ``dreaming''---which
represents a natural metaphorical mapping between human cognitive
processes and computational operations. Learning itself fundamentally
involves metaphorical operations when knowledge from one domain
scaffolds understanding in another. Perhaps most profoundly, the
metaphorical case provides a computational framework for understanding
how symbols and archetypes function in human cognition---as cross-domain
mappings that compress complex experiential patterns into transferable,
culturally-shared representations that retain their structural integrity
across diverse contexts while adapting to individual interpretive
frameworks.

\hypertarget{implications-of-novel-cases-for-computational-cognition}{%
\subsection{Implications of Novel Cases for Computational
Cognition}\label{implications-of-novel-cases-for-computational-cognition}}

The discovery of novel cases through CEREBRUM could have profound
implications for computational cognitive science:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Expanded representational capacity}: New cases enable
  representation of functional relationships beyond traditional
  linguistic frameworks
\item
  \textbf{Enhanced model compositionality}: Novel cases might enable
  more efficient composition of complex model assemblies
\item
  \textbf{Computational reflection}: Cases like the recursive case
  enable systematic implementation of self-modifying systems
\item
  \textbf{Cross-domain integration}: New cases like the metaphorical
  case might bridge domains that are difficult to connect with
  traditional case systems
\end{enumerate}

These speculative extensions of CEREBRUM highlight its potential not
just as an implementation of linguistic ideas in computational contexts,
but as a framework that could expand our understanding of functional
roles beyond traditional linguistic categories. The mathematical rigor
of CEREBRUM provides a foundation for systematically exploring this
expanded space of possible case functions, potentially leading to
entirely new paradigms for understanding complex model interactions in
cognitive systems.

\textbf{Table A1: Properties of Speculative Novel Cases in CEREBRUM}

\begin{longtable}[]{@{}llll@{}}
\toprule
\begin{minipage}[b]{0.11\columnwidth}\raggedright
Property\strut
\end{minipage} & \begin{minipage}[b]{0.26\columnwidth}\raggedright
Conjunctive Case {[}CNJ{]}\strut
\end{minipage} & \begin{minipage}[b]{0.24\columnwidth}\raggedright
Recursive Case {[}REC{]}\strut
\end{minipage} & \begin{minipage}[b]{0.27\columnwidth}\raggedright
Metaphorical Case {[}MET{]}\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\textbf{Function}\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright
Synthesizes multiple predictive streams into coherent joint predictions;
integrates diverse model outputs; resolves cross-model
inconsistencies\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Applies transformations to itself; enables self-modification; creates
meta-level processing loops\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Maps structures and relationships between domains; establishes
cross-domain correspondences; transfers knowledge patterns across
conceptual spaces\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\textbf{Parametric Focus}\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright
Cross-model correlation parameters and shared latent variables;
inter-model weights; joint distribution parameters\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Self-referential parameters; recursive transformations; meta-parameters
governing self-modification\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Structural alignment parameters; analogical mapping weights;
cross-domain correspondence metrics\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\textbf{Precision Weighting}\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright
Highest precision on inter-model consistency and joint predictions;
emphasizes mutual information; optimizes integration factors\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Dynamic self-allocation; recursive precision assignment; meta-precision
governing self-modification\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Selective precision on structural invariants; emphasis on relational
similarities over surface features; adaptive mapping precision\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\textbf{Interface Type}\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright
Aggregative interfaces with multiple connected models; convergent
communication channels; integration hubs\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Reflexive interfaces; self-directed connections; loopback channels\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Bridging interfaces across domain boundaries; cross-contextual mappings;
translation channels\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.11\columnwidth}\raggedright
\textbf{Update Dynamics}\strut
\end{minipage} & \begin{minipage}[t]{0.26\columnwidth}\raggedright
Updates based on joint prediction errors across the connected model
assembly; collective error minimization; consistency optimization\strut
\end{minipage} & \begin{minipage}[t]{0.24\columnwidth}\raggedright
Self-modification loops; introspective learning; meta-learning through
internal feedback\strut
\end{minipage} & \begin{minipage}[t]{0.27\columnwidth}\raggedright
Updates based on structural alignment success; transfer performance
feedback; analogical coherence optimization\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\backmatter
\end{document}

# Discussion on Language Complexity and Modeling (March 8, 2025)

## On Case Systems in Languages

> **Russian:**
> Ничего не понял. В языках падежи обычно отмирают, известный процесс. А тут почему они будут наоборот, появляться, а не отмирать? Что это даёт? Встроенное в язык удревление языка? Или экзотизация языка? В табасаранском языке (лезгинская ветвь нахско-дагестанской семьи языков ) 46 падежей — можно ли говорить, что он наиболее удачен для моделирования?
>
> **English:**
> I don't understand. In languages, cases usually die out - it's a known process. Why would they appear instead of disappearing? What does this give us? Built-in language archaization? Or language exoticization? The Tabasaran language (Lezgic branch of the Northeast Caucasian family) has 46 cases - can we say it's the most suitable for modeling?

> **Response:**
> - On language survival/fitness, I am not making claims about which kinds of languages (in terms of their number/type of cases, or any other linguistic features) are higher-fitness in general (as no such niche/context-independent fit-ness exists)
> - Rather CEREBRUM points towards a descriptive meta-language so we can describe languages with 3, 6, 18, 46, whatever number of cases (or verb tenses, other structures)
> - It sets up a context where we can do Bayes Factor driven model selection among which languages are prefered (of epistemic and pragmatic value)
> - So maybe there was misunderstanding, I did't (and don't) mean to advocate for so unilateral a view as, more cases is better
> - Rather I can totally see where fewer cases, could lead to e.g. more efficent or secure variable typing
> - Hence, why I sought an approach to understand and create case systems

## On Learning Complexity

> **Russian:**
> Учить английский — надо 700 часов, учить немецкий или русский — надо 1400 часов (есть статистика). В том числе из-за развитой системы падежей. Надо ли нам увеличивать время изучения языков моделирования?
>
> **English:**
> Learning English takes 700 hours, learning German or Russian takes 1400 hours (there are statistics). This is partly due to the developed case system. Should we increase the time needed to learn modeling languages?

> **Response:**
> - No. Again, I never said that more cases is better, or languages with more cases are better
> - Those differnt languages exist in a bigger space of symbolic coherences
> - What sets up the tradeoffs and fitness of a given syntax-semantics in an ecological niche, that is the "last mile" and system-specific work to be done (Low Road), existing amidst general principles (High Road) 

## On Unmotivated Complexity

> **Russian:**
> То есть вижу, чем с падежами будет хуже, не вижу, чтобы это как-то компенсировалось какой-то экономией хоть чего-нибудь — непонятно, чего именно эффективность растёт, IMHO просто растёт немотивированная сложность. То есть сначала надо рассказать, чем сложный язык лучше простого — и уже потом предлагать разные пути усложнения (там ведь не только падежи, там много чего ещё можно усложнить).
>
> **English:**
> I can see how cases make things worse, but I don't see how this is compensated by any savings - it's unclear what exactly becomes more efficient. In my opinion, it's just unmotivated complexity. First, we need to explain why a complex language is better than a simple one - and only then propose different ways to complicate it (there's more than just cases, there are many other things we could complicate).

> **Response:**
> - I never said a more complex language was better than less complex
> - Rather I am seeking to make a more general state space, from which simple and complex, actual and imaginary, languages can be described/designed
> - This allows us to find what is best (or manifolds of fits, etc) situationally, as a question of structure learning
> - It is because I want to understand what would describe or enable, that kind of situational difference, that I find such approaches as CEREBRUM, promising

## On Machine Translation Development

> **Russian:**
> Вспоминается известная поговорка разработчиков систем машинного перевода: "Каждый раз, когда мы увольняем из команды лингвиста, качество нашей системы перевода растёт" )))
>
> **English:**
> I'm reminded of the famous saying among machine translation system developers: "Every time we fire a linguist from the team, the quality of our translation system improves" )))

## On Current Modeling Trends

> **Russian:**
> Мне кажется, что основной ход на моделирование сейчас — это уход от языка как такового, переход к точкам в многомерном пространстве, внутренние представления нейросети, всякие ходы на представление latent space, вроде DroidSpeak и Coconut (и таких работ уже множество).
>
> **English:**
> It seems to me that the main trend in modeling now is moving away from language as such, transitioning to points in multidimensional space, neural network internal representations, various approaches to latent space representation, like DroidSpeak and Coconut (and there are already many such works).

> **Response:**
> - This approach works for custom inflections on latent space patterns and archetypes
> - It is not limited to only the natural langage cases, so I believe it will be useful across a variety of latent space types

## On Practical Value

> **Russian:**
> То есть без того, чтобы побить какую-нибудь SoTA, это не проходит. "Вот такую-то задачу решить сейчас могут с такой-то эффективностью, а с CEREBRUM можно поднять эффективность до вот такой". И если задачи нет, то надо придумать такую задачу — чтобы была видна польза (а не ценность, ибо ценность бывает и без пользы). Без задачи с очевидной пользой как-то не получается оценить по достоинству.
>
> **English:**
> Without beating some State-of-the-Art, this doesn't work. "This particular task can currently be solved with this level of efficiency, but with CEREBRUM we can increase efficiency to this level." And if there's no task, we need to come up with one - to make the benefit visible (not value, because value can exist without benefit). Without a task with obvious benefit, it's somehow impossible to properly evaluate it.

> **Response:**
> - Sure, what kind of tasks or SoTA do you suggest?
> - Then I can at least learn more about what types of benchmarks you think are key
> - Possibly even learn and make progress towards that benchmark itself
> - And/or come to deeper understanding of alternative tasks I consider more relevant, which I could then convey